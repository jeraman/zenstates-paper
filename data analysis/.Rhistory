#ABOUT DIFFERENT TYPES OF ANOVA
#https://www.researchgate.net/post/What_is_the_difference_between_1way_ANOVA_and_2way_ANOVA_and_MANOVA_How_would_I_know_where_to_apply_which_one
#OMNIBUS ONE WAY ANOVA ON DECISION SPEED
#http://rtutorialseries.blogspot.ca/2011/01/r-tutorial-series-one-way-anova-with.html
print(anova(lm(decision_accuracy ~ block, data=raw)))
##################
#different type of pairwise comparison
#https://www.r-bloggers.com/r-tutorial-series-anova-pairwise-comparison-methods/
##none
print(pairwise.t.test(raw$decision_accuracy, raw$block, p.adj="none"))
##bonferroni (used in proton)
print(pairwise.t.test(raw$decision_accuracy, raw$block, p.adj="bonferroni"))
##bonferroni (used in holm)
print(pairwise.t.test(raw$decision_accuracy, raw$block, p.adj="holm"))
##tukey pairwaise comparison
TukeyHSD(aov(decision_accuracy ~ block, data=raw))
}
subarray_of_logins_and_anwers <- function () {
#select unique values in subarray 7:16
subarray <- unique(raw[, 7:16])
#return result
return(subarray)
}
loading_one_user_file <- function(filename) {
#loading all file names
raw <- stream_in(file(filename))
#fp <- file.path("", filename)
#result <- fromJSON(file = fp)
raw = configuring_data_types(raw)
return(raw)
}
loading_all_user_filesA <- function() {
#loading all file names
filenames <- list.files("./rawdata", pattern="*.json", full.names=TRUE)
rm (raw)
rm(temp)
i = 0
raw = stream_in(file(filenames[1]))
#loading all files
for (i in 1:size) {
print(i)
temp <- stream_in(file(filenames[i]))
Sys.sleep(1)
if (i > 1) {
raw = rbind(raw,temp)
}
}
Summary(result)
raw = configuring_data_types(raw)
}
#example of combining two different files into one
loading_all_user_filesB <- function() {
rm(a)
rm(b)
rm(c)
rm(raw)
a <-loading_one_user_file("./rawdata/user1.json")
b <-loading_one_user_file("./rawdata/user2.json")
c <-loading_one_user_file("./rawdata/user3.json")
#View(a)
#View(b)
result = rbind(a,b,c)
result = configuring_data_types(result)
View(result)
}
configuring_data_types <-function(data) {
#converting to the right datatype
# answers
data$selectedanswer = as.character(data$selectedanswer);
data$rightanswer = as.character(data$rightanswer);
data$block = as.factor(data$block);
data$durationtime = as.numeric(data$durationtime);
data$videotime = as.numeric(data$videotime);
# login
data$id = as.character(data$id);
data$beginTimestamp = as.POSIXlt(data$beginTimestamp)
data$endTimestamp = as.POSIXlt(data$endTimestamp)
#profiling
data$age = as.numeric(data$age);
data$gender = as.factor(data$gender);
data$experience = as.numeric(data$experience);
data$experience = as.numeric(data$experience);
data$language = as.factor(data$language);
#final questionnaire
data$easier = as.factor(data$easier);
data$harder = as.factor(data$harder);
data = computing_decision_speed_and_accuracy(data)
return(data)
}
computing_decision_speed_and_accuracy <- function(data) {
#computing the decision time
data["decision_speed"] = data$durationtime - data$videotime
#computing if the answers were right
data["decision_accuracy"] = (data$selectedanswer == data$rightanswer)
return (data)
}
#from: http://vitalflux.com/data-science-scale-normalize-numeric-data-using-r/
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# this still have problems...
#analyzing_languages_per_id <-function () {
#getting the subarray
#    subarray = subarray_of_logins_and_anwers()
# loading the languages as a separate table
#    table_lang_id = subarray$languages
#getting unique ids
#    ids = unique(subarray$id)
# associating the ids to the languages
#    names(table_lang_id) <- subarray$id
# return results
#   return(table_lang_id)
#}
#example of how to deal with dates in r
example_on_how_to_deal_with_dates_in_r <- function () {
# converting to the proper format
dates = as.POSIXlt(raw$beginTimestamp)
# checking the array
dates
#ordering accord day and time
dates=dates[order(dates)]
# checking the ordered array
View(dates)
}
#example of how to deal with lists in R. this will be used to compute frequency in the languages
example_on_how_to_deal_with_lists_in_r_ <- function() {
# loading the profiles
raw <- stream_in(file("./profiling/user1-profiling.json"))
# loading the languages as a separate table
test = raw$languages
# associating the ids to the languages
names(test) <- raw$id
# seting the languages as factors
test=as.factor(test$id)
#showing a summary of the data
View(test)
}
#example of combining two different files into one
example_combining_different_files <- function() {
a <-loading_one_user_file("./rawdata/user1.json")
b <-loading_one_user_file("./rawdata/user3.json")
c <-loading_one_user_file("./rawdata/user4.json")
#View(a)
#View(b)
raw = rbind(a,b,c)
raw = rbind(b,c)
View(c)
}
#scatterplot
#with(only_right_answers, plot(decision_speed, block))
# hist(raw$decision_speed, col="green")
# rug(raw$decision_speed)
# plot(raw$durationtime)
computing_decision_errors_barplot_per_tool()
#############################
#############################
# Jeronimo Barbosa ##########
# August 14 2017 #############
#############################
# import json library
library(jsonlite)
#setting the workspace
setwd("~/Documents/dev/zenstates-media-backend/data analysis")
#reading one json file
raw <-loading_one_user_file("./pilot/p-user1.json")
raw <-loading_one_user_file("./pilot/p-user2.json")
raw <-loading_one_user_file("./pilot/p-user3.json")
raw <-loading_one_user_file("./pilot/p-user4.json")
raw <-loading_one_user_file("./pilot/p-user5.json")
raw <-loading_one_user_file("./pilot/p-user6.json")
raw <-loading_one_user_file("./rawdata/user3.json")
raw <-loading_one_user_file("./rawdata/user4.json")
raw <-loading_one_user_file("./rawdata/user5.json")
raw <-loading_one_user_file("./rawdata/user6.json")
raw <-loading_one_user_file("./rawdata/user7.json")
raw <-loading_one_user_file("./rawdata/user8.json")
raw <-loading_one_user_file("./rawdata/user9.json")
raw <-loading_one_user_file("./rawdata/user10.json")
raw <-loading_one_user_file("./rawdata/user11.json")
raw <-loading_one_user_file("./rawdata/user12.json")
raw <-loading_one_user_file("./rawdata/user13.json")
raw <-loading_one_user_file("./rawdata/user14.json")
#loading all file names
filenames <- list.files("./rawdata", pattern="*.json", full.names=TRUE)
filenames
#loading the first
raw = stream_in(file(filenames[1]))
#computing size
size = length(filenames)
print("loading...")
#loading all files
for (i in 1:size) {
print(i)
print(filenames[i])
temp <- stream_in(file(filenames[i]))
#temp["decision_speed"] = temp$durationtime - temp$videotime
#temp["decision_accuracy"] = (temp$selectedanswer == temp$rightanswer)
#temp["normalized_decision_speed"] <- normalize(temp$decision_speed)
Sys.sleep(1)
if (i > 1) {
raw = rbind(raw,temp)
}
}
#formating data types
raw = configuring_data_types(raw)
#summarizing data
summary(raw)
View(raw)
#press space bar + enter on the line you want to execute
computing_decision_speed_right_answers_boxplot_per_tool()
computing_decision_speed_all_answers_boxplot_per_tool()
computing_decision_errors_barplot_per_tool()
computing_average_completion_time()
detail_age()
detail_experience()
computing_decision_speed_barplot_with_sem_per_tool_all_answers()
detail_questionnaire()
decision_speed_anova_and_pairwise_tests()
decision_accuracy_anova_and_pairwise_tests()
## (one dimension data)
# plotting decision speed for everyone
#boxplot(raw$decision_speed, col="blue")
# drawing a line over the graph
#abline(h=median(raw$decision_speed))
#ploting categorical data
#barplot(table(raw$decision_accuraccy), col="wheat", main = "Decision accuracy")
## two dimentional data
#simple plot
#boxplot(raw$decision_speed ~ raw$block, col="blue")
#fancy plot
#boxplot(raw$decision_speed ~ raw$block, col="blue", xlab = "Conceptual model", ylab= "Decision time (in seconds)", names=c("Dataflow", "Imperative", "ZenStates"))
# function that computes a boxplot over the different evaluated tools
computing_decision_speed_right_answers_boxplot_per_tool <- function () {
# selecting a sebset of the original data (only the right ones)
only_right_answers = subset(raw, decision_accuracy == TRUE)
#ploting only the right ones
boxplot(decision_speed ~ block, only_right_answers, col="blue", xlab = "Conceptual model", ylab= "Decision time (in seconds)", names=c("Dataflow", "Imperative", "ZenStates"))
#drawing a line containing the average
abline(h=median(only_right_answers$decision_speed))
}
computing_decision_speed_all_answers_boxplot_per_tool <- function () {
# selecting a sebset of the original data (only the right ones)
#only_right_answers = subset(raw, decision_accuraccy == TRUE)
#ploting only the right ones
boxplot(decision_speed ~ block, raw, col=c("#fff7bc", "#fec44f", "#d95f0e"), ylim=c(0,100), names=c("Dataflow", "Imperative", "ZenStates"), frame.plot=FALSE, boxwex=.4)
title(xlab = expression(bold("Conceptual model")), ylab= expression(bold("Decision time (in seconds)")))
abline(h=median(raw$decision_speed), lwd=1, lty=2)
}
# function that computes a decision speed barplot per tool with standard error of the mean
computing_decision_speed_barplot_with_sem_per_tool_all_answers <- function () {
#getting raw data
data.dist <- split(raw$decision_speed, raw$block)
#renaming columns
names(data.dist)[names(data.dist)=="pd"]  <- "Dataflow"
names(data.dist)[names(data.dist)=="pde"] <- "Structured"
names(data.dist)[names(data.dist)=="zen"] <- "ZenStates"
#computing mean to order
data.dist.mean <- sapply(data.dist, mean)
#sorting main array
data.dist=data.dist[order(data.dist.mean,decreasing=FALSE)]
#computing mean again
data.dist.mean <- sapply(data.dist, mean)
#computing sd
sd <- sapply(data.dist, sd)
#computing length
length <- sapply(data.dist, length)
#computing sem
data.dist.sem <- sd / sqrt(length)
data.dist.sem
#ploting the graph
g = barplot(data.dist.mean, col=c("#fff7bc", "#fec44f", "#d95f0e"), ylim=c(0,100), space=0.5)
grid(nx=NA, ny=NULL, col = "black")
#zero line
abline(h=0, lwd=2, lty=1)
#ploting the error
arrows(x0=g, y0=data.dist.mean-data.dist.sem, x1=g, y1=data.dist.mean+data.dist.sem, lwd=.8, code=3, angle=90, length=0.15)
title(xlab = expression(bold("Conceptual model")), ylab= expression(bold("Decision time (in seconds)")))
title(main="Average decision time with standard error of the mean")
#formating digits
value=round(data.dist.mean, digits=2)
#printing actual values
text(g, 3, paste("n = ", value), cex=0.7)
#abline(h=mean(data.dist$pd), lwd=1, lty=2)
#abline(h=mean(data.dist$pde), lwd=1, lty=2)
#abline(h=mean(data.dist$zen), lwd=1, lty=2)
}
# function that computes a boxplot over the different evaluated tools
computing_decision_errors_barplot_per_tool <- function () {
# selecting a sebset of the original data (only the wrong ones)
only_wrong_answers = subset(raw, decision_accuracy == FALSE)
length(raw$decision_accuracy)/3
print("counting errors")
print(table(only_wrong_answers$block))
print("accuracy rate")
n=length(raw$decision_accuracy)/3
100-((table(only_wrong_answers$block)/n)*100)
#counting errors
#barplot(table(only_wrong_answers$block), col="wheat", main = "Decision errors", xlab = "Conceptual model", ylab= "Number of errors", names=c("Dataflow", "Imperative", "ZenStates"))
}
#computes average completion time
computing_average_completion_time <- function() {
return(median(raw$endTimestamp - raw$beginTimestamp))
}
#function that computes average age and standard deviation
detail_age <- function() {
print('AGE')
print(paste("mean", mean(raw$age)))
print(paste("median", median(raw$age)))
print(paste("sd", sd(raw$age)))
print(paste("min:", min(raw$age)))
print(paste("max age:", max(raw$age)))
}
#function that computes average experience and standard deviation
detail_experience <- function() {
print('EXPERIENCE')
print(paste("mean", mean(raw$experience)))
print(paste("median", median(raw$experience)))
print(paste("sd", sd(raw$experience)))
print(paste("min:", min(raw$experience)))
print(paste("max age:", max(raw$experience)))
#filtering one entry per id
sub = raw[match(unique(raw$id),raw$id),]
#counting
print("language frequency")
print(table(sub$language))
}
#details the answer of the questionnaire
detail_questionnaire <- function() {
#filtering one entry per id
sub = raw[match(unique(raw$id),raw$id),]
#View(sub)
#print(sub$easierwhy)
#counting
print("easier to understand")
print(table(sub$easier))
print("harder to understand")
print(table(sub$harder))
}
decision_speed_anova_and_pairwise_tests <- function () {
#ABOUT DIFFERENT TYPES OF ANOVA
#https://www.researchgate.net/post/What_is_the_difference_between_1way_ANOVA_and_2way_ANOVA_and_MANOVA_How_would_I_know_where_to_apply_which_one
#HOW TO REPORT THE ANOVA
#http://www.yorku.ca/mack/RN-HowToReportAnFStatistic.html
#OMNIBUS ONE WAY ANOVA ON DECISION SPEED
#http://rtutorialseries.blogspot.ca/2011/01/r-tutorial-series-one-way-anova-with.html
print(anova(lm(decision_speed ~ block, data=raw)))
##################
#different type of pairwise comparison
#https://www.r-bloggers.com/r-tutorial-series-anova-pairwise-comparison-methods/
##none
print(pairwise.t.test(raw$decision_speed, raw$block, p.adj="none"))
##bonferroni (used in proton)
print(pairwise.t.test(raw$decision_speed, raw$block, p.adj="bonferroni"))
##bonferroni (used in holm)
print(pairwise.t.test(raw$decision_speed, raw$block, p.adj="holm"))
##tukey pairwaise comparison
TukeyHSD(aov(decision_speed ~ block, data=raw))
}
decision_accuracy_anova_and_pairwise_tests <- function () {
#ABOUT DIFFERENT TYPES OF ANOVA
#https://www.researchgate.net/post/What_is_the_difference_between_1way_ANOVA_and_2way_ANOVA_and_MANOVA_How_would_I_know_where_to_apply_which_one
#OMNIBUS ONE WAY ANOVA ON DECISION SPEED
#http://rtutorialseries.blogspot.ca/2011/01/r-tutorial-series-one-way-anova-with.html
print(anova(lm(decision_accuracy ~ block, data=raw)))
##################
#different type of pairwise comparison
#https://www.r-bloggers.com/r-tutorial-series-anova-pairwise-comparison-methods/
##none
print(pairwise.t.test(raw$decision_accuracy, raw$block, p.adj="none"))
##bonferroni (used in proton)
print(pairwise.t.test(raw$decision_accuracy, raw$block, p.adj="bonferroni"))
##bonferroni (used in holm)
print(pairwise.t.test(raw$decision_accuracy, raw$block, p.adj="holm"))
##tukey pairwaise comparison
TukeyHSD(aov(decision_accuracy ~ block, data=raw))
}
subarray_of_logins_and_anwers <- function () {
#select unique values in subarray 7:16
subarray <- unique(raw[, 7:16])
#return result
return(subarray)
}
loading_one_user_file <- function(filename) {
#loading all file names
raw <- stream_in(file(filename))
#fp <- file.path("", filename)
#result <- fromJSON(file = fp)
raw = configuring_data_types(raw)
return(raw)
}
loading_all_user_filesA <- function() {
#loading all file names
filenames <- list.files("./rawdata", pattern="*.json", full.names=TRUE)
rm (raw)
rm(temp)
i = 0
raw = stream_in(file(filenames[1]))
#loading all files
for (i in 1:size) {
print(i)
temp <- stream_in(file(filenames[i]))
Sys.sleep(1)
if (i > 1) {
raw = rbind(raw,temp)
}
}
Summary(result)
raw = configuring_data_types(raw)
}
#example of combining two different files into one
loading_all_user_filesB <- function() {
rm(a)
rm(b)
rm(c)
rm(raw)
a <-loading_one_user_file("./rawdata/user1.json")
b <-loading_one_user_file("./rawdata/user2.json")
c <-loading_one_user_file("./rawdata/user3.json")
#View(a)
#View(b)
result = rbind(a,b,c)
result = configuring_data_types(result)
View(result)
}
configuring_data_types <-function(data) {
#converting to the right datatype
# answers
data$selectedanswer = as.character(data$selectedanswer);
data$rightanswer = as.character(data$rightanswer);
data$block = as.factor(data$block);
data$durationtime = as.numeric(data$durationtime);
data$videotime = as.numeric(data$videotime);
# login
data$id = as.character(data$id);
data$beginTimestamp = as.POSIXlt(data$beginTimestamp)
data$endTimestamp = as.POSIXlt(data$endTimestamp)
#profiling
data$age = as.numeric(data$age);
data$gender = as.factor(data$gender);
data$experience = as.numeric(data$experience);
data$experience = as.numeric(data$experience);
data$language = as.factor(data$language);
#final questionnaire
data$easier = as.factor(data$easier);
data$harder = as.factor(data$harder);
data = computing_decision_speed_and_accuracy(data)
return(data)
}
computing_decision_speed_and_accuracy <- function(data) {
#computing the decision time
data["decision_speed"] = data$durationtime - data$videotime
#computing if the answers were right
data["decision_accuracy"] = (data$selectedanswer == data$rightanswer)
return (data)
}
#from: http://vitalflux.com/data-science-scale-normalize-numeric-data-using-r/
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# this still have problems...
#analyzing_languages_per_id <-function () {
#getting the subarray
#    subarray = subarray_of_logins_and_anwers()
# loading the languages as a separate table
#    table_lang_id = subarray$languages
#getting unique ids
#    ids = unique(subarray$id)
# associating the ids to the languages
#    names(table_lang_id) <- subarray$id
# return results
#   return(table_lang_id)
#}
#example of how to deal with dates in r
example_on_how_to_deal_with_dates_in_r <- function () {
# converting to the proper format
dates = as.POSIXlt(raw$beginTimestamp)
# checking the array
dates
#ordering accord day and time
dates=dates[order(dates)]
# checking the ordered array
View(dates)
}
#example of how to deal with lists in R. this will be used to compute frequency in the languages
example_on_how_to_deal_with_lists_in_r_ <- function() {
# loading the profiles
raw <- stream_in(file("./profiling/user1-profiling.json"))
# loading the languages as a separate table
test = raw$languages
# associating the ids to the languages
names(test) <- raw$id
# seting the languages as factors
test=as.factor(test$id)
#showing a summary of the data
View(test)
}
#example of combining two different files into one
example_combining_different_files <- function() {
a <-loading_one_user_file("./rawdata/user1.json")
b <-loading_one_user_file("./rawdata/user3.json")
c <-loading_one_user_file("./rawdata/user4.json")
#View(a)
#View(b)
raw = rbind(a,b,c)
raw = rbind(b,c)
View(c)
}
#scatterplot
#with(only_right_answers, plot(decision_speed, block))
# hist(raw$decision_speed, col="green")
# rug(raw$decision_speed)
# plot(raw$durationtime)
